
IMAGE BIND: One Embedding Space To Bind Them All
Rohit Girdhar∗Alaaeldin El-Nouby∗Zhuang Liu Mannat Singh
Kalyan Vasudev Alwala Armand Joulin Ishan Misra∗
FAIR, Meta AI
https://facebookresearch.github.io/ImageBind
1) Cross-Modal Retrieval
Crackle of a Fire
AudioText
“A fire crackles while a pan of food is frying on the fire.”“Fire is crackling then wind starts blowing.”“Firewood crackles then music...”
“A baby is crying while a toddler is laughing.”“A baby is laughing while an adult is laughing.”“A baby laughs and something…”
Baby Cooing
Waves
2) Embedding-Space Arithmetic3) Audio to Image GenerationDogEngineFireImagesVideosDepth&
Rain
Figure 1. I MAGE BIND’s joint embedding space enables novel multimodal capabilities. By aligning six modalities’ embedding into a
common space, I MAGE BINDenables: 1)Cross-Modal Retrieval, which shows emergent alignment of modalities such as audio, depth or
text, that aren’t observed together. 2)Adding embeddings from different modalities naturally composes their semantics. And 3)Audio-to-
Image generation, by using our audio embeddings with a pre-trained DALLE-2 [61] decoder designed to work with CLIP text embeddings.
Abstract
We present IMAGE BIND, an approach to learn a joint
embedding across six different modalities - images, text, au-
dio, depth, thermal, and IMU data. We show that all combi-
nations of paired data are not necessary to train such a joint
embedding, and only image-paired data is sufficient to bind
the modalities together. IMAGE BIND can leverage recent
large scale vision-language models, and extends their zero-
shot capabilities to new modalities just by using their natu-
ral pairing with images. It enables novel emergent applica-
tions ‘out-of-the-box’ including cross-modal retrieval, com-
posing modalities with arithmetic, cross-modal detection
and generation. The emergent capabilities improve with the
strength of the image encoder and we set a new state-of-the-
art on emergent zero-shot recognition tasks across modal-
ities, outperforming specialist supervised models. Finally,
we show strong few-shot recognition results outperforming
prior work, and that IMAGE BINDserves as a new way to
evaluate vision models for visual and non-visual tasks.
∗Equal technical contribution.1. Introduction
A single image can bind together many experiences – an
image of a beach can remind us of the sound of waves, the
texture of the sand, a breeze, or even inspire a poem. This
‘binding’ property of images offers many sources of super-
vision to learn visual features, by aligning them with any
of the sensory experiences associated with images. Ideally,
for a single joint embedding space, visual features should
be learned by aligning to all of these sensors. However, this
requires acquiring all types and combinations of paired data
with the same set of images, which is infeasible.
Recently, many methods learn image features aligned
with text [1, 31, 46, 60, 64, 65, 82, 83], audio [3, 4, 50,
55, 56, 70] etc. These methods use a single pair of modal-
ities or, at best, a few visual modalities. However, the fi-
nal embeddings are limited to the pairs of modalities used
for training. Thus, video-audio embeddings cannot directly
be used for image-text tasks and vice versa. A major ob-
stacle in learning a true joint embedding is the absence of
large quantities of multimodal data where all modalities are
present together.arXiv:2305.05665v2  [cs.CV]  31 May 2023
In this paper, we present IMAGE BIND, which learns a
single shared representation space by leveraging multiple
types of image-paired data. It does not need datasets where
all modalities co-occur with each other. Instead, we lever-
age the binding property of images and we show that just
aligning each modality’s embedding to image embeddings
leads to an emergent alignment across all of the modalities.
In practice, I MAGE BINDleverages web-scale (image, text)
paired data and combines it with naturally occurring paired
data such as (video, audio), (image, depth) etc. to learn
a single joint embedding space. This allows I MAGE BIND
to implicitly align the text embeddings to other modalities
such as audio, depth etc., enabling zero-shot recognition ca-
pabilities on that modality without explicit semantic or tex-
tual pairing. Moreover, we show that it can be initialized
with large-scale vision-language models such as CLIP [60],
thereby leveraging the rich image and text representations
of these models. Thus, I MAGE BIND can be applied to a
variety of different modalities and tasks with little training.
We use large-scale image-text paired data along with nat-
urally paired ‘self-supervised’ data across four new modal-
ities - audio, depth, thermal, and Inertial Measurement Unit
(IMU) readings – and show strong emergent zero-shot clas-
sification and retrieval performance on tasks for each of
these modalities. These emergent properties improve as the
underlying image representation is made stronger. On au-
dio classification and retrieval benchmarks, I MAGE BIND’s
emergent zero-shot classification matches or outperforms
specialist models trained with direct audio-text supervision
on benchmarks like ESC, Clotho, AudioCaps. I MAGE BIND
representations also outperform specialist supervised mod-
els on few-shot evaluation benchmarks. Finally, we show
that I MAGE BIND’s joint embeddings can be used for a wide
variety of compositional tasks as illustrated in Figure 1, in-
cluding cross-modal retrieval, combining embeddings via
arithmetic, detecting audio sources in images, and generat-
ing images given audio input.
2. Related Work
IMAGE BIND builds upon several advances in vision-
language, multimodal, and self-supervised research.
Language Image Pre-training. Training images jointly
with linguistic signals like words or sentences has been
shown to be an effective method for zero-shot, open-
vocabulary recognition and text to image retrieval [14, 18,
38, 68]. Language as supervision can further be used for
learning strong video representations [2, 47, 48]. Joulin et
al. [34] show that using large-scale image dataset with noisy
captions yields strong visual features. Recently, CLIP [60],
ALIGN [31] and Florence [83] collect large collections of
image and text pairs and train models to embed image and
language inputs in a joint space using contrastive learning,
exhibiting impressive zero-shot performance. CoCa [82]adds an image captioning objective on top of the contrastive
loss for improved performance. Flamingo [1] handles arbi-
trarily interleaved images and texts, and achieves state of the
art on many few-shot learning benchmarks. LiT [84] adopts
contrastive training for fine-tuning and observes freezing
image encoders works the best. This prior line of works
mostly considers image and text, while our work enables
zero-shot recognition on multiple modalities.
Multi-Modal Learning. Our work binds multiple modal-
ity representations in a joint embedding space. Prior works
explored joint training of multiple modalities in a super-
vised [21, 42] or self-supervised contexts [3, 20, 50, 70, 74].
The success of image and language pre-training methods
such as CLIP has inspired approaches that revisits learn-
ing deep semantic representations through matching other
modalities with linguistic inputs. Various methods adapt
CLIP to extract semantically strong video representations
[15, 43, 45, 79]. Most related to our method, Nagrani et
al. [51] create a weakly-labeled dataset for paired video-
audio and captions that allows for training multi-modal
video-audio encoder to match textual features resulting in
strong audio and video retrieval and captioning perfor-
mance. AudioCLIP [27] adds audio as an additional modal-
ity into a CLIP framework, enabling zero-shot audio classi-
fication. In contrast, I MAGE BINDdoes not require explicit
paired data between all modalities and instead leverages im-
age as a natural weak supervision for unifying modalities.
Feature Alignment Pre-trained CLIP models have been
utilized as teachers to supervise other models due to the
strength of its visual representations [44, 58, 75]. More-
over, CLIP joint image and text embedding space has also
been leveraged for a variety of zero-shot tasks like de-
tection [24, 88], segmentation [41], mesh animation [81]
etc. showing the power of joint embedding spaces. Point-
CLIP [85] finds a pre-trained CLIP encoder can be used for
3D recognition by projecting a point cloud to a number of
2D depth map views, which in turn are encoded using CLIP
visual encoder. In multilingual neural machine translation,
a similar phenomenon to the emergence behavior of I M-
AGEBINDis commonly observed and utilized: if languages
are trained in the same latent space through learned implicit
bridging, translation can be done between language pairs on
which no paired data is provided [33, 40].
3. Method
Our goal is to learn a single joint embedding space for all
modalities by using images to bind them together. We align
each modality’s embedding to image embeddings, such as
text to image using web data and IMU to video using video
data captured from egocentric cameras with IMU. We show
that the resulting embedding space has a powerful emer-
gent zero-shot behavior that automatically associates pairs
of modalities without seeing any training data for that spe-
Web Image-Text
Thermal Data
Depth Sensor Data
IMAGEBIND
Images
Videos
TextAudio
Depth
Thermal
IMUNaturally AlignedEmergent AlignmentWeb Videos
Egocentric Videos
Figure 2. I MAGE BIND overview. Different modalities occur naturally aligned in different data sources, for instance images+text and
video+audio in web data, depth or thermal information with images, IMU data in videos captured with egocentric cameras, etc. IMAGE -
BINDlinks all these modalities in a common embedding space, enabling new emergent alignments and capabilities.
cific pair. We illustrate our approach in Figure 2.
3.1. Preliminaries
Aligning specific pairs of modalities. Contrastive learn-
ing [28] is a general technique for learning an embedding
space by using pairs of related examples (positives) and un-
related examples (negatives). Using pairs of aligned ob-
servations, contrastive learning can align pairs of modal-
ities such as (image, text) [60], (audio, text) [27], (image,
depth) [70], (video, audio) [50] etc. However, in each case,
the joint embeddings are trained and evaluated using the
same pairs of modalities. Thus, (video, audio) embeddings
are not directly applicable for text-based tasks while (image,
text) embeddings cannot be applied for audio tasks.
Zero-shot image classification using text prompts.
CLIP [60] popularized a ‘zero-shot’ classification task
based on an aligned (image, text) embedding space. This
involves constructing a list of text descriptions that describe
the classes in a dataset. An input image is classified based
on its similarity to the text descriptions in the embedding
space. Unlocking such zero-shot classification for other
modalities requires specifically training using paired text
data, e.g., (audio, text) [27] or (point-clouds, text) [85]. In
contrast, I MAGE BIND unlocks zero-shot classification for
modalities without paired text data.
3.2. Binding modalities with images
IMAGE BIND uses pairs of modalities ( I,M), where I
represents images and Mis another modality, to learn a sin-
gle joint embedding. We use large-scale web datasets with
(image, text) pairings that span a wide range of semantic
concepts. Additionally, we use the natural, self-supervised
pairing of other modalities – audio, depth, thermal, and In-
tertial Measurement Unit (IMU) – with images.
Consider the pair of modalities ( I,M) with aligned ob-
servations. Given an image Iiand its corresponding obser-
vation in the other modality Mi, we encode them into nor-
malized embeddings: qi=f(Ii)andki=g(Mi)where
f, gare deep networks. The embeddings and the encodersare optimized using an InfoNCE [54] loss:
LI,M=−logexp(q⊺
iki/τ)
exp(q⊺
iki/τ) +P
j̸=iexp(q⊺
ikj/τ),(1)
where τis a scalar temperature that controls the smoothness
of the softmax distribution and jdenotes unrelated observa-
tions, also called ‘negatives’. We follow [76] and consider
every example j̸=iin the mini-batch to be a negative. The
loss makes the embeddings qiandkicloser in the joint em-
bedding space, and thus aligns IandM. In practice, we
use a symmetric loss LI,M+LM,I.
Emergent alignment of unseen pairs of modalities. IM-
AGEBINDuses modalities paired with images, i.e., pairs of
the form ( I,M) to align each the embeddings from each
modality Mto those from images. We observe an emer-
gent behavior in the embedding space that aligns two pairs
of modalities (M1,M2)even though we only train using
the pairs (I,M1)and(I,M2). This behavior allows us
to perform a wide variety of zero-shot and cross-modal re-
trieval tasks without training for them. We achieve state-
of-the-art zero-shot text-audio classification results without
observing a single sample of paired (audio, text).
3.3. Implementation Details
IMAGE BIND is conceptually simple and can be imple-
mented in many different ways. We deliberately choose a
vanilla implementation that is flexible and allows for an ef-
fective study and easy adoption. In § 5, we present design
decisions that are critical for good emergent ‘binding’.
Encoding modalities. We use a Transformer architec-
ture [73] for all the modality encoders. We use the Vision
Transformer (ViT) [13] for images. Following [20], we use
the same encoder for images and videos. We temporally
inflate [7] the patch projection layer of the ViT and use 2
frame video clips sampled from 2seconds. We follow [22]
for encoding audio and convert a 2second audio sampled at
16kHz into spectrograms using 128mel-spectrogram bins.
As the spectrogram is also a 2D signal like an image, we use
a ViT with a patch size of 16and stride 10. We treat ther-
mal images and depth images as one-channel images and
Dataset Task #cls Metric #test
Audioset Audio-only (AS-A) [19] Audio cls. 527 mAP 19048
ESC 5-folds (ESC) [59] Audio cls. 50 Acc 400
Clotho (Clotho) [17] Retrieval - Recall 1045
AudioCaps (AudioCaps) [37] Retrieval - Recall 796
VGGSound (VGGS) [8] Audio cls. 309 Acc 14073
SUN Depth-only (SUN-D) [69] Scene cls. 19 Acc 4660
NYU-v2 Depth-only (NYU-D) [66] Scene cls. 10 Acc 653
LLVIP (LLVIP) [32] Person cls. 2 Acc 15809
Ego4D (Ego4D) [23] Scenario cls. 108 Acc 68865
Table 1. Emergent zero-shot classification datasets for audio,
depth, thermal, and Inertial Measurement Unit (IMU) modalities.
We evaluate I MAGE BINDwithout training for any of these tasks
and without training on paired text data for these modalities. For
each dataset, we report the task (classification or retrieval), number
of classes (#cls), metric for evaluation (Accuracy or mean Average
Precision), and the number of test samples (#test).
also use a ViT to encode them. We follow [21] to convert
depth into disparity maps for scale invariance. We extract
the IMU signal consisting of accelerometer and gyroscope
measurements across the X,Y, and Zaxes. We use 5sec-
ond clips resulting in 2K time step IMU readings which are
projected using a 1D convolution with a kernel size of 8.
The resulting sequence is encoded using a Transformer. Fi-
nally, we follow the text encoder design from CLIP [60].
We use separate encoders for images, text, audio, ther-
mal images, depth images, and IMU. We add a modality-
specific linear projection head on each encoder to obtain a
fixed size ddimensional embedding, that is normalized and
used in the InfoNCE loss from Eq 1. In addition to ease of
learning, this setup allows us to also initialize a subset of
the encoders using pretrained models, e.g., the image and
text encoder using CLIP [60] or OpenCLIP [30].
4. Experiments
We first describe the main experimental setup and pro-
vide full details in the supplement.
Naturally paired modalities and datasets. We use I M-
AGEBIND on six modalities - image/video, text, audio,
depth, thermal images, and IMU. As described in § 3.3, we
treat videos as 2 frame images and process them the same
as images. For the naturally available paired data, we use
the (video, audio) pairs from the Audioset dataset [19], (im-
age, depth) pairs from the SUN RGB-D dataset [69], (im-
age, thermal) pairs from the LLVIP dataset [32] and (video,
IMU) pairs from the Ego4D dataset [23]. For these pairs of
modalities, we do not use any extra supervision like class la-
bels, text etc. Since SUN RGB-D and LLVIP are relatively
small, we follow [21] and replicate them 50 ×for training.
Large scale image-text pairs. We leverage image-text su-
pervision from large-scale web data [60]. For ease of ex-
perimentation, we use pretrained models that are trained
on billions of (image, text) pairs. Specifically, we use thepretrained vision (ViT-H 630M params) and text encoders
(302M params) from OpenCLIP [11, 30].
Encoders for each modality. We convert audio into 2D
mel-spectrograms [22], and thermal and depth modalities
into 1 channel images and use ViT-B, ViT-S encoders re-
spectively. The image and text encoders are kept frozen
during the I MAGE BINDtraining and the audio, depth, ther-
mal, and IMU encoders are updated.
Emergent zero-shot vs. zero-shot. Methods such as
CLIP [60], AudioCLIP [27] etc. train with modality pairs,
(image, text) and (audio, text), to demonstrate zero-shot
classification using text-prompts for the same modality. In
contrast, I MAGE BINDbinds modalities together using only
image-paired data. Thus, just by training on (image, text)
and (image, audio), I MAGE BIND can perform zero-shot
classification of audio using text prompts. As we do not
directly train for this ability, we term it emergent zero-shot
classification to distinguish it from methods that specifically
train using paired text-supervision for all modalities.
Evaluation on downstream tasks. We comprehensively
evaluate I MAGE BIND on a many different downstream
tasks using different protocols. We summarize the main
datasets used for evaluation in Table 1.
4.1. Emergent zero-shot classification
We evaluate I MAGE BINDon emergent zero-shot classi-
fication and use the text prompt templates from [60] (full
details in Appendix B). We report the results in Table 2.
Each task measures I MAGE BIND’s ability to associate text
embeddings to the other modalities without observing them
together during training. Given the novelty of our problem
setting, there are no “fair” baselines to compare I MAGE -
BIND with. Nevertheless, we compare to prior work that
uses text paired with certain modalities ( e.g. audio [27, 51]),
and for certain “visual-like” modalities such as depth and
thermal, we use the CLIP model directly. We also report
the best reported supervised upper bound per benchmark.
IMAGE BIND achieves a high emergent zero-shot clas-
sification performance. On each benchmark, I MAGE BIND
achieves strong gains and even compares favorably to super-
vised specialist models trained for the specific modality and
task. These results demonstrate that I MAGE BINDaligns the
modalities and implicitly transfers the text supervision as-
sociated with images to other modalities like audio. In par-
ticular, I MAGE BINDshows strong alignment for non-visual
modalities like audio and IMU suggesting that their natu-
rally available pairing with images is a powerful source of
supervision. For completeness, we also report the standard
zero-shot image (ImageNet [63] - IN1K, Places-365 [87] -
P365) and video (Kinetics400 [35] - K400, MSR-VTT 1k-
A [78] - MSR-VTT) tasks. As the image & text encoders
are initialized (and frozen) using OpenCLIP, these results
match those of OpenCLIP.
IN1K P365 K400 MSR-VTT NYU-D SUN-D AS-A VGGS ESC LLVIP Ego4D
Random 0.1 0.27 0.25 0.1 10.0 5.26 0.62 0.32 2.75 50.0 0.9
IMAGE BIND 77.7 45.4 50.0 36.1 54.0 35.1 17.6 27.8 66.9 63.4 25.0
Text Paired - - - - 41.9∗25.4∗28.4†[27] - 68.6†[27] - -
Absolute SOTA 91.0 [82] 60.7 [67] 89.9 [80] 57.7 [79] 76.7 [21] 64.9 [21] 49.6 [39] 52.5 [36] 97.0 [9] - -
Table 2. Emergent zero-shot classification of IMAGE BINDusing text prompts highlighted in blue. I MAGE BINDaligns images with text,
depth, audio, thermal and IMU modalities. The resulting embedding space can associate text embeddings with the non-image modalities,
and leads to strong emergent zero-shot classification. We show strong performance even on non-visual modalities such as audio and IMU.
We compare to ‘Text Paired’ baselines wherever possible, which trains with paired text data for that modality.∗We use the OpenCLIP ViT-
H [30] on depth rendered as grayscale images.†[27] that uses AS class names as supervision during training, and hence is not “zero-shot”.
Overall, I MAGE BINDshows strong emergent zero-shot performance, even compared to such upper bounds. We also report the absolute
state-of-the-art (SOTA) on each dataset for reference, which typically uses additional supervision, model ensembles etc. We report the
top-1 classification accuracy for all datasets except MSR-VTT (Recall@1) and Audioset Audio-only (mAP).
Emergent Clotho AudioCaps ESC
R@1 R@10 R@1 R@10 Top-1
Uses audio and text supervision
AudioCLIP [27] ✗ 68.6
Uses audio and text loss
A VFIC [51] ✗ 3.0 17.5 8.7 37.7
No audio and text supervision
IMAGE BIND ✓ 6.0 28.4 9.3 42.3 66.9
Supervised
A VFIC finetuned [51] ✗ 8.4 38.6
ARNLQ [53] ✗ 12.6 45.4 24.3 72.1
Table 3. Emergent zero-shot audio retrieval and classification.
We compare I MAGE BIND to prior work on zero-shot audio re-
trieval and audio classification. Without using audio-specific su-
pervision, I MAGE BIND outperforms prior methods on zero-shot
retrieval and has comparable performance on the classification
task. I MAGE BIND’s emergent zero-shot performance approaches
those of specialist supervised models.
4.2. Comparison to prior work
We now compare I MAGE BIND against prior work in
zero-shot retrieval and classification tasks.
Zero-shot text to audio retrieval and classification. Un-
like I MAGE BIND, prior work trains using paired data for
that modality, e.g., AudioCLIP [27] uses (audio, text) su-
pervision and A VFIC [52] uses automatically mined (au-
dio, text) pairs. We compare their zero-shot text to audio
retrieval and classification performance to I MAGE BIND’s
emergent retrieval and classification in Table 3.
IMAGE BINDsignificantly outperforms prior work on the
audio text retrieval benchmarks. On the Clotho dataset, I M-
AGEBINDhas double the performance of A VFIC despite not
using any text pairing for audio during training. Compared
to the supervised AudioCLIP model, I MAGE BINDachieves
comparable audio classification performance on ESC. Note
that AudioCLIP uses class names from AudioSet as text
targets for audio-text training, hence is referred to as ‘su-Modality Emergent MSR-VTT
R@1 R@5 R@10
MIL-NCE [49] V ✗ 8.6 16.9 25.8
SupportSet [57] V ✗ 10.4 22.2 30.0
FIT [5] V ✗ 15.4 33.6 44.1
A VFIC [51] A+V ✗ 19.4 39.5 50.3
IMAGE BIND A ✓ 6.8 18.5 27.2
IMAGE BIND A+V ✗ 36.8 61.8 70.0
Table 4. Zero-shot text based retrieval on MSR-VTT 1K-A.
We compare I MAGE BIND’s emergent retrieval performance using
audio and observe that it performs favorably to methods that use
the stronger video modality for retrieval.
pervised’. I MAGE BIND’s strong performance on all three
benchmarks validates its ability to align the audio and text
modalities using images as a bridge.
Text to audio and video retrieval. We use the MSR-VTT
1k-A benchmark to evaluate the text to audio and video re-
trieval performance in Table 4. Only using audio, I MAGE -
BINDachieves strong emergent retrieval performance com-
pared to the video retrieval performance of prior work like
MIL-NCE. The text to video performance for our model is
strong (36.1% R@1 in Table 2) as it uses OpenCLIP’s vi-
sion and text encoders and outperforms many prior meth-
ods. However, combining the audio and video modalities
further boosts performance showing the utility of I MAGE -
BIND’s features over an already strong retrieval model.
4.3. Few-shot classification
We now evaluate the label-efficiency of I MAGE BINDby
evaluating on few-shot classification. We use the audio and
depth encoders from I MAGE BINDand evaluate them on au-
dio and depth classification respectively in Figure 3. For
≥1-shot results, we follow [50, 60] and train linear classi-
fiers on fixed features (details in Appendix B).
On few-shot audio classification (Figure 3 left), we com-
pare with (1) self-supervised AudioMAE model trained
012 4 820406080
# shots per classESC (Fold-1) Top-1IMAGE BIND
AudioMAE [77]
Supervised [77]
012 4 810203040
# shots per classSUN-D Top-1
IMAGE BIND
MultiMAE [4]
Figure 3. Few-shot classification on audio and depth. We report
the emergent zero-shot classification performance on each bench-
mark (denoted by ⋆). We train linear classifiers on fixed features
for the≥1-shot case. (Left) In all settings, I MAGE BINDoutper-
forms the self-supervised AudioMAE model. I MAGE BIND even
outperforms a supervised AudioMAE model upto 4 shot learning
showing its strong generalization. (Right) We compare with the
MultiMAE model trained with images, depth, and semantic seg-
mentation masks. I MAGE BINDoutperforms MultiMAE across all
few-shot settings on few-shot depth classification.
on audio from Audioset and (2) a supervised AudioMAE
model finetuned on audio classification. Both baselines
use the same capacity ViT-B audio encoder as I MAGE -
BIND. I MAGE BIND significantly outperforms the Au-
dioMAE model on all settings with gains of ∼40% accuracy
in top-1 accuracy on ≤4-shot classification. I MAGE BIND
also matches or outperforms the supervised model on ≥1-
shot classification. I MAGE BIND’s emergent zero-shot per-
formance surpasses the supervised ≤2-shot performance.
For few-shot depth classification, we compare with the
multimodal MultiMAE [4] ViT-B/16 model trained on im-
ages, depth, and semantic segmentation data. I MAGE BIND
significantly outperforms MultiMAE across all the few-shot
settings. Altogether, these results show the strong gener-
alization of I MAGE BIND audio and depth features trained
with image alignment.
4.4. Analysis and Applications
Multimodal embedding space arithmetic. We study
whether I MAGE BIND’s embeddings can be used to com-
pose information across modalities. In Figure 4, we show
image retrievals obtained by adding together image and au-
dio embeddings. The joint embedding space allows for us to
compose two embeddings: e.g., image of fruits on a table +
sound of chirping birds and retrieve an image that contains
both these concepts, i.e., fruits on trees with birds. Such
emergent compositionality whereby semantic content from
different modalities can be composed will likely enable a
rich variety of compositional tasks.
Without re-training, we can ‘upgrade’ existing vision
models that use CLIP embeddings to use I MAGE BINDem-
beddings from other modalities such as audio.
Upgrading text-based detectors to audio-based. We use a
Claps
Church Bells
Chirping birds
Thunderstorm
Figure 4. Embedding space arithmetic where we add image
and audio embeddings, and use them for image retrieval. The
composed embeddings naturally capture semantics from different
modalities. Embeddings from an image of fruits + the sound of
birds retrieves images of birds surrounded by fruits.
Dog barking
Keyboard typingClock alarmSea waves
Figure 5. Object detection with audio queries. Simply replacing
Detic [88]’s CLIP-based ‘class’ embeddings with our audio em-
beddings leads to an object detector promptable with audio. This
requires no re-training of any model.
pretrained text-based detection model, Detic [88], and sim-
ply replace its CLIP-based ‘class’ (text) embeddings with
IMAGE BIND’s audio embeddings. Without training, this
creates an ‘audio’-based detector that can detect and seg-
ment objects based on audio prompts. As shown in Fig-
ure 5, we can prompt the detector with the barking sound of
a dog to localize a dog.
Upgrading text-based diffusion models to audio-based.
We use a pretrained DALLE-2 [61] diffusion model (private
reimplementation) and replace its prompt embeddings by
our audio embeddings. In Figure 1, we observe that we can
repurpose the diffusion model to generate plausible images
using different types of sounds.
5. Ablation Study
We investigate various design choices for learning a joint
embedding space for different modalities. Since the abla-
tion experimental setup is similar to § 4, we only note the
main differences (full details in Appendix C). We report re-
sults on the ESC fold-1 for the ablation study. We use a ViT-
B encoder for the image, audio, depth, and thermal modali-
ties by default and train them for 16 epochs ( vs. 32 epochs
ViT-B ViT-L ViT-H44464850NYU-D
ViT-B ViT-L ViT-H6264ESC Fold-1
ViT-B ViT-L ViT-H586062LLVIP
ViT-B ViT-L ViT-H18202224Ego4D
Figure 6. Scaling the image encoder size while keeping the other
modality encoders’ size fixed. We measure the performance on
the emergent zero-shot classification of depth, audio, thermal, and
IMU modalities. Scaling the image encoder significantly improves
the zero-shot classification results suggesting that a stronger visual
representation improves the ‘binding’ of modalities.
in § 4). For IMU we use a lightweight 6 layer encoder
with 512 dimensional width and 8 heads, and train it for 8
epochs. The text encoder follows [60] and is a twelve layer
Transformer with a width of 512 dimensions. We initialize
the image and text encoder from the CLIP model [60].
5.1. Scaling the Image Encoder
The central idea in I MAGE BINDis aligning the embed-
dings of all modalities to image embeddings. Thus, the im-
age embeddings plays a central role in the emergent align-
ment of unseen modalities and we study their effect on the
emergent zero-shot performance. We vary the size of the
image encoder and train an encoder for the depth, audio
etc. modalities to match the image representation. To iso-
late the effect of the image representation, we fix the size
of the other modality encoders. We use the pretrained CLIP
(ViT-B and ViT-L) and OpenCLIP (ViT-H) image and text
encoders for this experiment. Our results in Figure 6 show
that I MAGE BIND’s emergent zero-shot performance on all
modalities improves with better visual features. For depth
and audio classification, the stronger ViT-H vs. the ViT-B
image encoder, provides a gain of 7% and 4% respectively.
Thus, stronger visual features can improve recognition per-
formance even on non-visual modalities.
5.2. Training Loss and Architecture
We study the effect of the training design choices on the
emergent zero-shot classification. We focus on two modali-
ties with different characteristics - depth which is visual and
spatial, and audio which is non-visual and has a temporal
component. We found that studying these diverse modali-
ties led to robust and transferable design decisions.
Contrastive loss temperature. We study the effect of thetemperature τ( Eq 1) in Table 5a. We experiment with a
learnable temperature initialized to 0.07(parametrized in
the log-scale) following [60] vs. various values of fixed tem-
peratures. Unlike [60], we observe that a fixed temperature
is best for depth, audio and IMU classification. Addition-
ally, we see that a higher temperature is better for train-
ing the depth, thermal, and IMU encoders, whereas a lower
temperature works best for the audio modality.
Projection head. We vary the projection head used for each
encoder from a linear layer to an MLP with 768hidden di-
mensions. The results in Table 5b show that a linear pro-
jection performs better for both modalities. This is in con-
trast to standard self-supervised methods like SimCLR [10]
whose performance improves with MLP projection heads.
Training epochs. We vary the number training epochs and
report the classification performance in Table 5c. Longer
training consistently improves the emergent zero-shot per-
formance for both modalities across all datasets.
Data augmentation for paired images. During I M-
AGEBIND training, we augment images either using ba-
sic augmentation (cropping, color jitter) or strong aug-
mentation that additionally applies RandAugment [12] and
RandErase [86]. We specify the augmentation parameters
in Appendix C. Stronger augmentation helps depth classifi-
cation when training on the small number of (image, depth)
pairs from the SUN RGB-D dataset. However, for audio,
strongly augmenting the video makes the task too challeng-
ing, leading to a significant drop of 34% on ESC.
Depth specific design choices. We vary the type of spatial
crops used for training in Table 5e. Following CMC [70],
we use two unaligned random crops from the correspond-
ing image and depth pair vs. our default choice of using
spatially aligned random crops. Contrary to CMC, we ob-
serve that random cropping severely degrades performance:
more than 10% on SUN-D. Unlike vanilla self-supervised
learning, our image representations learned from image-
text pairs are more semantic and thus spatially misaligned
crops hurt performance. In Table 5f, we observe that Ran-
domErase [86] boosts performance on depth classification.
Audio specific design choices. We train for video-audio
alignment using temporally aligned samples or unaligned
samples and measure the final performance in Table 5g.
Similar to the depth classification observation, temporally
aligned samples lead to better performance. Table 5h shows
that using frequency masking augmentation for audio also
provides a small boost in performance.
Capacity of the audio and depth encoders and their im-
pact of the classification performance is reported in Table 6.
A smaller encoder for depth improves performance pre-
sumably because of the relatively small size of the (image,
depth) dataset. Conversely, we observe that larger audio en-
coder improves the performance, particularly when paired
with a high capacity image encoder.
Temp→Learn 0.05 0.07 0.2 1.0
SUN-D 24.1 27.0 27.3 26.7 28.0
ESC 54.8 56.7 52.4 45.4 24.3
(a) Temperature for loss .Proj head →Linear MLP
SUN-D 26.7 26.5
ESC 56.7 51.0
(b) Projection Head .Epochs →16 32 64
SUN-D 26.7 27.9 29.9
ESC 56.7 61.3 62.9
(c) Training epochs .Data aug →Basic Strong
SUN-D 25.4 26.7
ESC 56.7 22.6
(d) Data aug for image .
Spatial align →None Aligned
SUN-D 16.0 26.7
(e) Spatial alignment of depth .Data aug →None RandErase
SUN-D 24.2 26.7
(f) Depth data aug .Temporal align →None Aligned
ESC 55.7 56.7
(g) Temporal alignment of audio .Data aug →Basic +Freq mask
ESC 56.5 56.7
(h) Audio data aug .
Table 5. Training loss and architecture design decisions and their impact on emergent zero-shot classification. Settings for results in § 4
highlighted in gray. (a)A fixed temperature in the contrastive loss outperforms a learnable one for all modalities. (b)A linear projection
head for computing the depth or audio embedding works better than an MLP head. (c)Longer training improves the zero-shot classification
performance for both modalities. (d)Stronger image augmentation improves depth classification while basic augmentation significantly
improves audio classification. (e, f) Using spatially aligned image and depth crops when training I MAGE BIND significantly improves
performance. Similarly, RandErase augmentation is critical to good zero-shot classification on depth. (g, h) Temporally aligned audio and
video matching gives improved performance and using frequency augmentation for audio gives a slight improvement.
Audio Encoder (ESC) Depth Encoder (SUN)
Image Encoder ViT-S ViT-B ViT-S ViT-B
ViT-B 52.8 56.7 30.7 26.7
ViT-H 54.8 60.3 33.3 29.5
Table 6. Capacity of the audio and depth encoders and their
impact on performance. A stronger image encoder improves per-
formance for both audio and depth tasks. As the number of (image,
depth) pairs is small, a smaller encoder improves performance for
depth. For audio classification, a larger encoder is better.
Batch size → 512 1k 2k 4k
NYU-D 47.3 46.5 43.0 39.9
ESC 39.4 53.9 56.7 53.9
Table 7. Effect of scaling batch size. We found the optimal batch
size for contrastive loss varied by the modality. For image-depth
task, a smaller batch size was better, likely due to the small size
and limited diversity of the original dataset. For audio-video task
where we have a lot more positive and negative audio-video pairs,
using a large batch size lead to better results.
Effect of batch size. In Table 7 we evaluate the effect of
batch size on the representation learned. As shown, the
batch size can vary across modalities depending on the size
and complexity of the corresponding pretraining datasets.
IMAGE BINDto evaluate pretrained vision models in Ta-
ble 8. We initialize the vision encoder using a pretrained
model and keep it fixed. We use image-paired data to align
and train text, audio, and depth encoders (full details in Ap-
pendix B). Compared to the supervised DeiT model, the
self-supervised DINO model is better at emergent zero-shot
classification on both depth and audio modalities. More-
over, the emergent zero-shot performance is not correlated
with the pure vision performance on ImageNet suggest-
ing that these tasks measure different properties. I MAGE -
BINDcan serve as a valuable tool to measure vision models’
strength on multimodal applications.IN1K VGGS ESC SUN-D NYU-D
DINO [6] 64.4 17.2 44.7 26.8 48.8
DeiT [72] 74.4†9.6 25.0 25.2 48.0
Table 8. I MAGE BIND as an evaluation tool. We initialize (and
fix) the image encoder with different methods and align other
modalities. I MAGE BIND measures the impact of visual features
on multimodal tasks.†trained with IN1K supervision.
6. Discussion and Limitations
IMAGE BINDis a simple and practical way to train a joint
embedding space using only image alignment. Our method
leads to emergent alignment across all modalities which
can be measured using cross-modal retrieval and text-based
zero-shot tasks. We enable a rich set of compositional mul-
timodal tasks across different modalities, show a way to
evaluate pretrained vision models for non-vision tasks and
‘upgrade’ models like Detic and DALLE-2 to use using au-
dio. There are multiple ways to further improve I MAGE -
BIND. Our image alignment loss can be enriched by using
other alignment data, for instance other modalities paired
with text, or with each other ( e.g. audio with IMU). Our
embeddings are trained without a specific downstream task,
and thus lag the performance of specialist models. More re-
search into adapting general purpose embeddings for each
task, including structured prediction tasks such as detection
will be beneficial. Finally, new benchmarks, e.g. our emer-
gent zero-shot task to measure emergent abilities of multi-
modal models, would help create exciting new applications.
Our model is a research prototype and cannot be readily
used for real world applications ( Appendix F).
Acknowledgements: Authors would like to thank Uriel
Singer, Adam Polyak and Naman Goyal for their help with
the DALLE-2 experiments, and the entire Meta AI team for
many helpful discussions.
References
[1] Jean-Baptiste Alayrac, Jeff Donahue, Pauline Luc, An-
toine Miech, Iain Barr, Yana Hasson, Karel Lenc, Arthur
Mensch, Katie Millican, Malcolm Reynolds, Roman Ring,
Eliza Rutherford, Serkan Cabi, Tengda Han, Zhitao Gong,
Sina Samangooei, Marianne Monteiro, Jacob Menick, Se-
bastian Borgeaud, Andrew Brock, Aida Nematzadeh, Sa-
hand Sharifzadeh, Mikolaj Binkowski, Ricardo Barreira,
Oriol Vinyals, Andrew Zisserman, and Karen Simonyan.
Flamingo: a visual language model for few-shot learning.
InNeurIPS , 2022. 1, 2
[2] Jean-Baptiste Alayrac, Adria Recasens, Rosalia Schneider,
Relja Arandjelovic, Jason Ramapuram, Jeffrey De Fauw, Lu-
cas Smaira, Sander Dieleman, and Andrew Zisserman. Self-
supervised multimodal versatile networks. NeurIPS , 2020.
2
[3] Relja Arandjelovic and Andrew Zisserman. Look, listen and
learn. In ICCV , 2017. 1, 2
[4] Roman Bachmann, David Mizrahi, Andrei Atanov, and Amir
Zamir. MultiMAE: Multi-modal Multi-task Masked Autoen-
coders. In ECCV , 2022. 1, 6
[5] Max Bain, Arsha Nagrani, G ¨ul Varol, and Andrew Zisser-
man. Frozen in time: A joint video and image encoder for
end-to-end retrieval. In ICCV , 2021. 5
[6] Mathilde Caron, Hugo Touvron, Ishan Misra, Herv ´e J´egou,
Julien Mairal, Piotr Bojanowski, and Armand Joulin. Emerg-
ing properties in self-supervised vision transformers. In
ICCV , 2021. 8
[7] Jo ˜ao Carreira and Andrew Zisserman. Quo vadis, action
recognition? A new model and the kinetics dataset. In CVPR ,
2017. 3
[8] Honglie Chen, Weidi Xie, Andrea Vedaldi, and Andrew Zis-
serman. Vggsound: A large-scale audio-visual dataset. In
ICASSP , 2020. 4, 12
[9] Ke Chen, Xingjian Du, Bilei Zhu, Zejun Ma, Taylor Berg-
Kirkpatrick, and Shlomo Dubnov. Hts-at: A hierarchical
token-semantic audio transformer for sound classification
and detection. In ICASSP , 2022. 5
[10] Ting Chen, Simon Kornblith, Mohammad Norouzi, and Ge-
offrey Hinton. A simple framework for contrastive learning
of visual representations. In ICML , 2020. 7, 14
[11] Mehdi Cherti, Romain Beaumont, Ross Wightman, Mitchell
Wortsman, Gabriel Ilharco, Cade Gordon, Christoph Schuh-
mann, Ludwig Schmidt, and Jenia Jitsev. Reproducible scal-
ing laws for contrastive language-image learning. In CVPR ,
2023. 4
[12] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V
Le. Randaugment: Practical automated data augmentation
with a reduced search space. In CVPR , 2020. 7
[13] Alexey Dosovitskiy, Lucas Beyer, Alexander Kolesnikov,
Dirk Weissenborn, Xiaohua Zhai, Thomas Unterthiner,
Mostafa Dehghani, Matthias Minderer, Georg Heigold, Syl-
vain Gelly, et al. An image is worth 16x16 words: Trans-
formers for image recognition at scale. In ICLR , 2021. 3
[14] Fartash Faghri, David J Fleet, Jamie Ryan Kiros, and Sanja
Fidler. VSE++: Improving Visual-Semantic Embeddings
with Hard Negatives. In BMVC , 2018. 2
[15] Han Fang, Pengfei Xiong, Luhui Xu, and Yu Chen.
Clip2video: Mastering video-text retrieval via image clip.arXiv preprint arXiv:2106.11097 , 2021. 2
[16] Christoph Feichtenhofer, Haoqi Fan, Yanghao Li, and Kaim-
ing He. Masked autoencoders as spatiotemporal learners. In
NeurIPS , 2022. 13
[17] Frederic Font, Gerard Roma, and Xavier Serra. Freesound
technical demo. In ACM MM , 2013. 4, 12
[18] Andrea Frome, Greg S Corrado, Jon Shlens, Samy Bengio,
Jeff Dean, Marc’Aurelio Ranzato, and Tomas Mikolov. De-
vise: A deep visual-semantic embedding model. NeurIPS ,
2013. 2
[19] Jort F. Gemmeke, Daniel P. W. Ellis, Dylan Freedman, Aren
Jansen, Wade Lawrence, R. Channing Moore, Manoj Plakal,
and Marvin Ritter. Audio set: An ontology and human-
labeled dataset for audio events. In ICASSP , 2017. 4, 12
[20] Rohit Girdhar, Alaaeldin El-Nouby, Mannat Singh,
Kalyan Vasudev Alwala, Armand Joulin, and Ishan Misra.
OmniMAE: Single Model Masked Pretraining on Images
and Videos. In CVPR , 2023. 2, 3
[21] Rohit Girdhar, Mannat Singh, Nikhila Ravi, Laurens van der
Maaten, Armand Joulin, and Ishan Misra. Omnivore: A Sin-
gle Model for Many Visual Modalities. In CVPR , 2022. 2,
4, 5, 12
[22] Yuan Gong, Yu-An Chung, and James Glass. AST: Audio
Spectrogram Transformer. In Interspeech , 2021. 3, 4, 13
[23] Kristen Grauman, Andrew Westbury, Eugene Byrne,
Zachary Chavis, Antonino Furnari, Rohit Girdhar, Jackson
Hamburger, Hao Jiang, Miao Liu, Xingyu Liu, Miguel Mar-
tin, Tushar Nagarajan, Ilija Radosavovic, Santhosh Kumar
Ramakrishnan, Fiona Ryan, Jayant Sharma, Michael Wray,
Mengmeng Xu, Eric Zhongcong Xu, Chen Zhao, Siddhant
Bansal, Dhruv Batra, Vincent Cartillier, Sean Crane, Tien
Do, Morrie Doulaty, Akshay Erapalli, Christoph Feichten-
hofer, Adriano Fragomeni, Qichen Fu, Abrham Gebrese-
lasie, Cristina Gonzalez, James Hillis, Xuhua Huang, Yifei
Huang, Wenqi Jia, Weslie Khoo, Jachym Kolar, Satwik Kot-
tur, Anurag Kumar, Federico Landini, Chao Li, Yanghao
Li, Zhenqiang Li, Karttikeya Mangalam, Raghava Modhugu,
Jonathan Munro, Tullie Murrell, Takumi Nishiyasu, Will
Price, Paola Ruiz Puentes, Merey Ramazanova, Leda Sari,
Kiran Somasundaram, Audrey Southerland, Yusuke Sugano,
Ruijie Tao, Minh V o, Yuchen Wang, Xindi Wu, Takuma
Yagi, Ziwei Zhao, Yunyi Zhu, Pablo Arbelaez, David Cran-
dall, Dima Damen, Giovanni Maria Farinella, Christian Fue-
gen, Bernard Ghanem, Vamsi Krishna Ithapu, C. V . Jawahar,
Hanbyul Joo, Kris Kitani, Haizhou Li, Richard Newcombe,
Aude Oliva, Hyun Soo Park, James M. Rehg, Yoichi Sato,
Jianbo Shi, Mike Zheng Shou, Antonio Torralba, Lorenzo
Torresani, Mingfei Yan, and Jitendra Malik. Ego4d: Around
the world in 3,000 hours of egocentric video. In CVPR , 2022.
4, 12
[24] Xiuye Gu, Tsung-Yi Lin, Weicheng Kuo, and Yin Cui.
Open-vocabulary object detection via vision and language
knowledge distillation. In ICLR , 2022. 2
[25] Saurabh Gupta, Pablo Arbelaez, and Jitendra Malik. Per-
ceptual organization and recognition of indoor scenes from
rgb-d images. In CVPR , 2013. 13
[26] Saurabh Gupta, Ross Girshick, Pablo Arbel ´aez, and Jitendra
Malik. Learning rich features from rgb-d images for object
detection and segmentation. In ECCV , 2014. 13
[27] Andrey Guzhov, Federico Raue, J ¨orn Hees, and Andreas
Dengel. AudioCLIP: Extending CLIP to Image, Text and
Audio. arXiv preprint arXiv:2106.13043 , 2021. 2, 3, 4, 5
[28] Raia Hadsell, Sumit Chopra, and Yann LeCun. Dimension-
ality reduction by learning an invariant mapping. In CVPR ,
2006. 3
[29] Gao Huang, Yu Sun, Zhuang Liu, Daniel Sedra, and Kilian Q
Weinberger. Deep networks with stochastic depth. In ECCV ,
2016. 14
[30] Gabriel Ilharco, Mitchell Wortsman, Ross Wightman, Cade
Gordon, Nicholas Carlini, Rohan Taori, Achal Dave,
Vaishaal Shankar, Hongseok Namkoong, John Miller, Han-
naneh Hajishirzi, Ali Farhadi, and Ludwig Schmidt. Open-
clip, 2021. 4, 5
[31] Chao Jia, Yinfei Yang, Ye Xia, Yi-Ting Chen, Zarana Parekh,
Hieu Pham, Quoc Le, Yun-Hsuan Sung, Zhen Li, and Tom
Duerig. Scaling up visual and vision-language representation
learning with noisy text supervision. In ICML , 2021. 1, 2
[32] Xinyu Jia, Chuang Zhu, Minzhen Li, Wenqi Tang, and Wenli
Zhou. Llvip: A visible-infrared paired dataset for low-light
vision. In ICCV , 2021. 4, 12, 13
[33] Melvin Johnson, Mike Schuster, Quoc V Le, Maxim Krikun,
Yonghui Wu, Zhifeng Chen, Nikhil Thorat, Fernanda Vi ´egas,
Martin Wattenberg, Greg Corrado, Macduff Hughes, and Jef-
frey Dean. Google’s multilingual neural machine translation
system: Enabling zero-shot translation. In ACL, 2017. 2
[34] Armand Joulin, Laurens van der Maaten, Allan Jabri, and
Nicolas Vasilache. Learning visual features from large
weakly supervised data. In ECCV , 2016. 2
[35] Will Kay, Joao Carreira, Karen Simonyan, Brian Zhang,
Chloe Hillier, Sudheendra Vijayanarasimhan, Fabio Viola,
Tim Green, Trevor Back, Paul Natsev, Mustafa Suleyman,
and Andrew Zisserman. The kinetics human action video
dataset. arXiv preprint arXiv:1705.06950 , 2017. 4
[36] Evangelos Kazakos, Arsha Nagrani, Andrew Zisserman, and
Dima Damen. Slow-fast auditory streams for audio recogni-
tion. In ICASSP , 2021. 5
[37] Chris Dongjoo Kim, Byeongchang Kim, Hyunmin Lee, and
Gunhee Kim. Audiocaps: Generating captions for audios in
the wild. In NAACL , 2019. 4, 12
[38] Ryan Kiros, Ruslan Salakhutdinov, and Richard S Zemel.
Unifying visual-semantic embeddings with multimodal neu-
ral language models. In NeurIPS Workshop , 2014. 2
[39] Khaled Koutini, Jan Schl ¨uter, Hamid Eghbal-zadeh, and Ger-
hard Widmer. Efficient training of audio transformers with
patchout. In Interspeech , 2022. 5
[40] Guillaume Lample, Alexis Conneau, Ludovic Denoyer, and
Marc’Aurelio Ranzato. Unsupervised machine translation
using monolingual corpora only. In ICLR , 2018. 2
[41] Boyi Li, Kilian Q Weinberger, Serge Belongie, Vladlen
Koltun, and Ren ´e Ranftl. Language-driven semantic seg-
mentation. In ICLR , 2022. 2
[42] Valerii Likhosherstov, Anurag Arnab, Krzysztof Choroman-
ski, Mario Lucic, Yi Tay, Adrian Weller, and Mostafa De-
hghani. Polyvit: Co-training vision transformers on images,
videos and audio. arXiv preprint arXiv:2111.12993 , 2021. 2
[43] Ziyi Lin, Shijie Geng, Renrui Zhang, Peng Gao, Gerard de
Melo, Xiaogang Wang, Jifeng Dai, Yu Qiao, and Hongsheng
Li. Frozen clip models are efficient video learners. In ECCV ,2022. 2
[44] Xingbin Liu, Jinghao Zhou, Tao Kong, Xianming Lin, and
Rongrong Ji. Exploring target representations for masked
autoencoders. arXiv preprint arXiv:2209.03917 , 2022. 2
[45] Huaishao Luo, Lei Ji, Ming Zhong, Yang Chen, Wen Lei,
Nan Duan, and Tianrui Li. CLIP4Clip: An Empirical Study
of CLIP for End to End Video Clip Retrieval. arXiv preprint
arXiv:2104.08860 , 2021. 2
[46] Dhruv Mahajan, Ross Girshick, Vignesh Ramanathan,
Kaiming He, Manohar Paluri, Yixuan Li, Ashwin Bharambe,
and Laurens Van Der Maaten. Exploring the limits of weakly
supervised pretraining. In ECCV , 2018. 1
[47] Antoine Miech, Jean-Baptiste Alayrac, Lucas Smaira, Ivan
Laptev, Josef Sivic, and Andrew Zisserman. End-to-end
learning of visual representations from uncurated instruc-
tional videos. In CVPR , 2020. 2
[48] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. ICCV , 2019. 2
[49] Antoine Miech, Dimitri Zhukov, Jean-Baptiste Alayrac,
Makarand Tapaswi, Ivan Laptev, and Josef Sivic.
Howto100m: Learning a text-video embedding by watching
hundred million narrated video clips. In ICCV , 2019. 5
[50] Pedro Morgado, Nuno Vasconcelos, and Ishan Misra. Audio-
visual instance discrimination with cross-modal agreement.
InCVPR , 2021. 1, 2, 3, 5
[51] Arsha Nagrani, Paul Hongsuck Seo, Bryan Seybold, Anja
Hauth, Santiago Manen, Chen Sun, and Cordelia Schmid.
Learning audio-video modalities from image captions. In
ECCV , 2022. 2, 4, 5
[52] Arsha Nagrani, Shan Yang, Anurag Arnab, Aren Jansen,
Cordelia Schmid, and Chen Sun. Attention bottlenecks for
multimodal fusion. In NeurIPS , 2021. 5
[53] Andreea-Maria Oncescu, A Koepke, Joao F Henriques,
Zeynep Akata, and Samuel Albanie. Audio retrieval with
natural language queries. In Interspeech , 2021. 5, 12
[54] Aaron van den Oord, Yazhe Li, and Oriol Vinyals. Rep-
resentation learning with contrastive predictive coding. In
NeurIPS , 2018. 3
[55] Andrew Owens and Alexei A Efros. Audio-visual scene
analysis with self-supervised multisensory features. In
ECCV , 2018. 1
[56] Mandela Patrick, Yuki M Asano, Ruth Fong, Jo ˜ao F Hen-
riques, Geoffrey Zweig, and Andrea Vedaldi. Multi-modal
self-supervision from generalized data transformations. In
ICCV , 2021. 1
[57] Mandela Patrick, Po-Yao Huang, Yuki Asano, Florian
Metze, Alexander Hauptmann, Joao Henriques, and Andrea
Vedaldi. Support-set bottlenecks for video-text representa-
tion learning. In ICLR , 2021. 5
[58] Zhiliang Peng, Li Dong, Hangbo Bao, Qixiang Ye,
and Furu Wei. BEiT v2: Masked image modeling
with vector-quantized visual tokenizers. arXiv preprint
arXiv:2208.06366 , 2022. 2
[59] Karol J Piczak. Esc: Dataset for environmental sound clas-
sification. In ACM MM , 2015. 4, 12
[60] Alec Radford, Jong Wook Kim, Chris Hallacy, Aditya
Ramesh, Gabriel Goh, Sandhini Agarwal, Girish Sastry,
Amanda Askell, Pamela Mishkin, Jack Clark, et al. Learn-
ing transferable visual models from natural language super-
vision. In ICML , 2021. 1, 2, 3, 4, 5, 7, 13, 14, 15
[61] Aditya Ramesh, Prafulla Dhariwal, Alex Nichol, Casey Chu,
and Mark Chen. Hierarchical text-conditional image gen-
eration with clip latents. arXiv preprint arXiv:2204.06125 ,
2022. 1, 6, 13
[62] Ren ´e Ranftl, Katrin Lasinger, David Hafner, Konrad
Schindler, and Vladlen Koltun. Towards robust monocular
depth estimation: Mixing datasets for zero-shot cross-dataset
transfer. TPAMI , 2020. 13
[63] Olga Russakovsky, Jia Deng, Hao Su, Jonathan Krause, San-
jeev Satheesh, Sean Ma, Zhiheng Huang, Andrej Karpathy,
Aditya Khosla, Michael Bernstein, Alexander C. Berg, and
Li Fei-Fei. ImageNet Large Scale Visual Recognition Chal-
lenge. IJCV , 2015. 4
[64] Christoph Schuhmann, Romain Beaumont, Richard Vencu,
Cade Gordon, Ross Wightman, Mehdi Cherti, Theo
Coombes, Aarush Katta, Clayton Mullis, Mitchell Worts-
man, Patrick Schramowski, Srivatsa Kundurthy, Katherine
Crowson, Ludwig Schmidt, Robert Kaczmarczyk, and Jenia
Jitsev. LAION-5B: An open large-scale dataset for training
next generation image-text models. In NeurIPS Datasets and
Benchmarks , 2022. 1
[65] Christoph Schuhmann, Richard Vencu, Romain Beaumont,
Robert Kaczmarczyk, Clayton Mullis, Aarush Katta, Theo
Coombes, Jenia Jitsev, and Aran Komatsuzaki. Laion-400m:
Open dataset of clip-filtered 400 million image-text pairs. In
NeurIPS Workshop , 2021. 1
[66] Nathan Silberman, Derek Hoiem, Pushmeet Kohli, and Rob
Fergus. Indoor segmentation and support inference from
rgbd images. In ECCV , 2012. 4, 12, 13
[67] Mannat Singh, Laura Gustafson, Aaron Adcock, Vinicius
de Freitas Reis, Bugra Gedik, Raj Prateek Kosaraju, Dhruv
Mahajan, Ross Girshick, Piotr Doll ´ar, and Laurens van der
Maaten. Revisiting weakly supervised pre-training of visual
perception models. In CVPR , 2022. 5
[68] Richard Socher, Andrej Karpathy, Quoc V Le, Christopher D
Manning, and Andrew Y Ng. Grounded compositional se-
mantics for finding and describing images with sentences.
ACL, 2014. 2
[69] Shuran Song, Samuel P Lichtenberg, and Jianxiong Xiao.
Sun rgb-d: A rgb-d scene understanding benchmark suite. In
CVPR , 2015. 4, 12
[70] Yonglong Tian, Dilip Krishnan, and Phillip Isola. Con-
trastive multiview coding. arXiv preprint arXiv:1906.05849 ,
2019. 1, 2, 3, 7
[71] Zhan Tong, Yibing Song, Jue Wang, and Limin Wang.
Videomae: Masked autoencoders are data-efficient learners
for self-supervised video pre-training. In NeurIPS , 2022. 13
[72] Hugo Touvron, Matthieu Cord, Matthijs Douze, Francisco
Massa, Alexandre Sablayrolles, and Herv ´e J´egou. Training
data-efficient image transformers & distillation through at-
tention. In ICML , 2021. 8
[73] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszko-
reit, Llion Jones, Aidan N Gomez, Lukasz Kaiser, and Illia
Polosukhin. Attention is all you need. In NeurIPS , 2017. 3
[74] Rui Wang, Dongdong Chen, Zuxuan Wu, Yinpeng Chen,
Xiyang Dai, Mengchen Liu, Yu-Gang Jiang, Luowei Zhou,and Lu Yuan. BEVT: Bert pretraining of video transformers.
InCVPR , 2022. 2
[75] Yixuan Wei, Han Hu, Zhenda Xie, Zheng Zhang, Yue Cao,
Jianmin Bao, Dong Chen, and Baining Guo. Contrastive
learning rivals masked image modeling in fine-tuning via
feature distillation. arXiv preprint arXiv:2205.14141 , 2022.
2
[76] Zhirong Wu, Yuanjun Xiong, Stella X Yu, and Dahua Lin.
Unsupervised feature learning via non-parametric instance
discrimination. In CVPR , 2018. 3
[77] Hu Xu, Juncheng Li, Alexei Baevski, Michael Auli, Woj-
ciech Galuba, Florian Metze, and Christoph Feichtenhofer.
Masked autoencoders that listen. In NeurIPS , 2022. 6
[78] Jun Xu, Tao Mei, Ting Yao, and Yong Rui. Msr-vtt: A large
video description dataset for bridging video and language. In
CVPR , 2016. 4
[79] Hongwei Xue, Yuchong Sun, Bei Liu, Jianlong Fu, Ruihua
Song, Houqiang Li, and Jiebo Luo. CLIP-ViP: Adapting Pre-
trained Image-Text Model to Video-Language Representa-
tion Alignment. In ICLR , 2023. 2, 5
[80] Shen Yan, Xuehan Xiong, Anurag Arnab, Zhichao Lu, Mi
Zhang, Chen Sun, and Cordelia Schmid. Multiview trans-
formers for video recognition. In CVPR , 2022. 5
[81] Kim Youwang, Kim Ji-Yeon, and Tae-Hyun Oh. Clip-actor:
Text-driven recommendation and stylization for animating
human meshes. In ECCV , 2022. 2
[82] Jiahui Yu, Zirui Wang, Vijay Vasudevan, Legg Yeung, Mo-
jtaba Seyedhosseini, and Yonghui Wu. Coca: Contrastive
captioners are image-text foundation models. TMLR , 2022.
1, 2, 5
[83] Lu Yuan, Dongdong Chen, Yi-Ling Chen, Noel Codella,
Xiyang Dai, Jianfeng Gao, Houdong Hu, Xuedong Huang,
Boxin Li, Chunyuan Li, Ce Liu, Mengchen Liu, Zicheng Liu,
Yumao Lu, Yu Shi, Lijuan Wang, Jianfeng Wang, Bin Xiao,
Zhen Xiao, Jianwei Yang, Michael Zeng, Luowei Zhou, and
Pengchuan Zhang. Florence: A new foundation model for
computer vision. arXiv preprint arXiv:2111.11432 , 2021. 1,
2
[84] Xiaohua Zhai, Xiao Wang, Basil Mustafa, Andreas Steiner,
Daniel Keysers, Alexander Kolesnikov, and Lucas Beyer.
Lit: Zero-shot transfer with locked-image text tuning. In
CVPR , 2022. 2
[85] Renrui Zhang, Ziyu Guo, Wei Zhang, Kunchang Li, Xu-
peng Miao, Bin Cui, Yu Qiao, Peng Gao, and Hongsheng
Li. Pointclip: Point cloud understanding by clip. In CVPR ,
2022. 2, 3
[86] Zhun Zhong, Liang Zheng, Guoliang Kang, Shaozi Li, and
Yi Yang. Random erasing data augmentation. In AAAI , 2020.
7
[87] Bolei Zhou, Agata Lapedriza, Jianxiong Xiao, Antonio Tor-
ralba, and Aude Oliva. Learning deep features for scene
recognition using places database. In NeurIPS , 2014. 4
[88] Xingyi Zhou, Rohit Girdhar, Armand Joulin, Philipp
Kr¨ahenb ¨uhl, and Ishan Misra. Detecting twenty-thousand
classes using image-level supervision. In ECCV , 2022. 2, 6,
13
A. Datasets and Metrics
Audioset (AS) [19]. This dataset is used for both training
and evaluation. It contains 10s videos from YouTube anno-
tated into 527 classes. It consists of 3 pre-defined splits, the
balanced split with about 20K videos, test split with 18K
videos, and an unbalanced training split with about 2M vi-
does. For training , we use the 2M unbalanced set without
any labels, and only use it for audio-video matching. For
zero-shot evaluation in Table 2, we use the test set and
compute logits for each class using the textual class names
along with the templates as described later in Appendix B.3.
The metric used is top-1 accuracy.
ESC-50 (ESC) [59]. We use this dataset for evaluating the
learned representations in a zero-shot manner. The task here
is “Environmental Sound Classification” (ESC). It consists
of 2000 5s audio clips classified into 50 classes. It has pre-
defined 5 fold evaluation, each consisting of 400 test audio
clips. In this work, we compute 0-shot predictions on the
evaluation set for each fold and report the 5-fold average
performance. For ablations we use only the first fold for
computational ease. The metric used is top-1 accuracy.
Clotho (Clotho) [17]. This is a dataset of audio from the
Freesound platform with textual descriptions. It consists of
a dev and test set of 2893 and 1045 audio clips respectively,
with each clip associated with 5 descriptions. We consider
the text →audio retrieval task, and consider each of the 5 as-
sociated captions as a separate test query and retrieve from
the set of audio clips. The metric used is recall@ K, where
a given test query is assumed to be correctly solved if the
ground truth audio is retrieved within the top- Kretrieved
audio clips.
AudioCaps (AudioCaps) [37]. This is a dataset of audio-
visual clips from YouTube accompanied by textual descrip-
tions. It consists of clips from the Audioset dataset as de-
scribed earlier. We use the splits as provided in [53],1which
removes clips that overlap with the VGGSound dataset. We
end up with 48198 training, 418 validation and 796 test
clips. We only use the test set for zero-shot evaluation of
our model. The task is text →audio retrieval, and evaluation
is performed using recall@ K.
VGGSound (VGGS) [8]. This dataset contains about 200K
video clips of 10s length, annotated with 309 sound classes
consisting of human actions, sound-emitting objects and
human-object interactions. We only use the audio from the
test set (with 14073 clips) for 0-shot classification. The
evaluation is done using the top-1 accuracy metric.
SUN RGB-D (SUN). We use the registered RGB and Depth
maps provided in the SUN RGB-D [69] dataset train set
(∼5K pairs) for training our model. We follow [21] to post
process the depth maps in two steps - 1) we use in-filled
1https : / / www . robots . ox . ac . uk / ˜vgg / research / audio -
retrieval/resources/benchmark- files/AudioCaps_retrieval_
dataset.tar.gzdepth values and 2) convert them to disparity for scale nor-
malization. This dataset is only used in training, so we do
not use any metadata or class labels.
SUN Depth-only (SUN-D). We use only the ∼5K depth
maps from the val split of the SUN RGB-D [69] dataset
and denote them as SUN Depth-only. This dataset is only
used for evaluation and we do not use the RGB images. We
process the depth maps similar to SUN RGB-D (in-filled
depth, converted to disparity). We use the 19 scene classes
in the dataset and use their class names for constructing the
zero-shot classification templates.
NYU-v2 Depth-only (NYU-D). We use the 794 val set
depth maps from the NYU-v2 Depth-only [66] dataset
for evaluation only. We post-process the depth similar to
SUN Depth-only. We use the 10 scene class names in the
dataset. The 10th scene class, called ‘other’, correspond to
18 different semantic classes – [’basement’, ’cafe’,
’computer lab’, ’conference room’, ’dinette’,
’exercise room’, ’foyer’, ’furniture store’,
’home storage’, ’indoor balcony’, ’laundry
room’, ’office kitchen’, ’playroom’, ’printer
room’, ’reception room’, ’student lounge’,
’study’, ’study room’] . For zero-shot evaluation,
we compute the cosine similarity of the 10th class as the
maximum cosine similarity among these 18 classnames.
LLVIP (LLVIP). The LLVIP dataset [32] consists of RGB
image and Thermal (infrared low-light) image pairs. The
dataset was collected in an outdoor setting using fixed cam-
eras observing street scenes and contains RGB images taken
in a low-light paired with infrared images (8 ∼14um fre-
quency). The RGB thermal pairs are registered in the
dataset release. For training, we use the train set with
12025 RGB image and thermal pairs. For evaluation,
we use the val set with 3463 pairs of RGB and ther-
mal images. Since the original dataset is designed for
detection, we post process it for a binary classification
task. We crop out pedestrian bounding boxes and ran-
dom bounding boxes (same aspect ratio and size as pedes-
trian) to create a balanced set of 15809 total boxes (7931
‘person’ boxes). For zero-shot classification, we use the
following class names for the ‘person’ class - [’per-
son’, ’man’, ’woman’, ’people’] , and [’street’,
’road’, ’car’, ’light’, ’tree’] for the background
class.
Ego4D (Ego4D) [23]. For the Ego4D dataset, we consider
the task of scenario classification. There are 108 unique sce-
narios present in the 9,645 videos of the Ego4D dataset. We
filter out all videos annotated with more than one scenario
which yields 7,485 videos with a single scenario assigned.
For each video, We select all time-stamps that contains a
synchronized IMU signal as well as aligned narrations. We
sample 5 second clips around each time-stamp. The dataset
is split randomly such that we have 510,142 clips for train-
ing, and 68,865 clips for testing. During training we only
use the video frames and their corresponding IMU signal.
We use the test split to measure zero-shot scenario classi-
fication performance, where each clip of IMU signal is as-
signed the video-level scenario label as its ground-truth.
A.1. Data Representations
We use the standard RGB and RGBT representations
forimages and videos . For videos, we use 2-frame clips,
inspired from recent work on ViT-style video architec-
tures [16, 71], where a video patch is 2×16×16(T×H×W).
We inflate the visual encoder’s weights to work with spa-
tiotemporal patches and and at inference time we aggregate
features over multiple 2-frame clips. Hence, we can use
models trained on image-text data directly on videos.
We used a single-channel image for the thermal data
since it is the natural form in which current infrared thermal
sensors return data [32]. For single-view depth , we ex-
perimented with different encodings – absolute depth [66]
as returned by sensors like the Kinect, inverse depth [62],
disparity [62], and HHA [25, 26]. Overall, we found that
disparity representation (which is a single-channel image)
worked the best. For audio we use the raw waveform pro-
cessed into mel-spectrograms [22], as described in the main
text. For IMU we use a 6×Ttensor to represent the se-
quence of IMU sensor readings over time.
B. Evaluation details
We now describe the evaluation setups used in this work.
B.1. Inference implementation details
Audio/Video: For both these temporal modalities (whether
operated upon together during pre-training or separately
during inference), we sample fixed length clips to operate
on. During training, we randomly sample a clip, typically
2s in length. At inference time, we uniformly sample multi-
ple clips to cover the full length of the input sample. For in-
stance, for 5s ESC videos, we would sample ⌈5
2⌉= 3clips.
For video clips, we sample a fixed number of frames from
each clip. For audio, we process each raw audio waveform
by sampling it at 16KHz followed by extracting a log mel
spectrogram with 128 frequency bins using a 25ms Ham-
ming window with hop length of 10ms. Hence, for a tsec-
ond audio we get a 128×100tdimensional input.
IMU: For IMU, we sample fixed length clips of 5 seconds,
centered around time-stamps that are aligned with narra-
tions. For each clip, we get a 6×2000 dimensional input and
we measure the zero-shot performance for scenario classifi-
cation using each clip as an independent testing sample.
B.2. Few-shot evaluation details
For the few-shot results in Figures 3 using the ESC and
SUN datasets, we sampled ktraining samples per class,where k∈ {1,2,4,8}. We fix the ksamples such that
our model and the baselines use exactly the same samples
during training. For all few-shot evaluations, including the
baselines, we freeze the encoder parameters and only train
a linear classifier.
Audio: For audio few-shot training with ESC, our model
and the baselines are trained using AdamW with a learning
rate of 1.6×10−3and weight decay of 0.05for 50 epochs.
Depth: For depth few-shot training with SUN, our model
and the baselines are trained using AdamW with a learning
rate of 10−2and no weight decay for 60 epochs.
B.3. Zero-shot evaluation details
Query Templates. For all evaluations, we use the default
set of templates from CLIP [60].2Note that we use the same
templates for non visual modalities like audio and depth as
well since we only use semantic/textual supervision associ-
ated with images.
B.4. Qualitative evaluation details
Cross-modal nearest neighbors. We perform the re-
trieval on the embedding feature after temperature scaling.
The nearest neighbors are computed using cosine distance.
In Figure 1, we show retrievals for audio from ESC, image
retrievals from IN1K and COCO, depth from SUN-D, and
text from AudioCaps.
Embedding arithmetic. For arithmetic, we again use the
embedding features after temperature scaling. We ℓ2nor-
malize the features and sum the embeddings after scaling
them by 0.5. We use the combined feature to perform near-
est neighbor retrieval using cosine distance, as described
above. In Figure 1, we show combination of images and
audio from IN1K and ESC, and show retrievals from IN1K.
Audio→Image Generation. For generating images form
audio clips, we rely on an in-house reproduced implemen-
tation of DALLE-2 [61]. In DALLE-2, to produce images
from text prompts, the image generation model relies on
text embeddings produced by the pre-trained CLIP-L/14
text encoder. Since I MAGE BIND naturally aligns CLIP’s-
embedding space to that of other modalities proposed in the
paper, we can upgrade the DALLE-2 model to generate im-
ages by prompting it with these new unseen modalities. We
achieve zero-shot audio to image generation with DALLE-2
by simply using the temperature-scaled audio embeddings
generated by I MAGE BIND’s audio encoder as a proxy for
the CLIP’s text embeddings in the DALLE-2’s image gen-
eration model.
Detecting objects using audio. We extract all audio de-
scriptors from the validation set of ESC using an I MAGE -
BIND ViT-B/32 encoder, yielding 400 descriptors in total.
We use an off-the-shelf CLIP-based Detic [88] model and
2https://github.com/openai/CLIP/blob/main/notebooks/
Prompt_Engineering_for_ImageNet.ipynb
use the audio descriptors as the classifier for Detic in place
of CLIP text-based ‘class’ embeddings. We use a score
threshold of 0.9 for the qualitative results in Figure 5.
C. Pretraining details
C.1. Best setup
In Table 9 we detail the hyperparameters used to pre-
train each of the models reported in Table 4. Our experi-
ments were done on 32GB V100 or 40GB A100 GPUs.
Config AS SUN LLVIP Ego4D
Vision encoder ViT-Huge
embedding dim. 768 384 768 512
number of heads 12 8 12 8
number of layers 12 12 12 6
Optimizer AdamW
Optimizer Momentum β1= 0.9, β2= 0.95
Peak learning rate 1.6e-3 1.6e-3 5e-4 5e-4
Weight decay 0.2 0.2 0.05 0.5
Batch size 2048 512 512 512
Gradient clipping 1.0 1.0 5.0 1.0
Warmup epochs 2
Sample replication 1.25 50 25 1.0
Total epochs 64 64 64 8
Stoch. Depth [29] 0.1 0.0 0.0 0.7
Temperature 0.05 0.2 0.1 0.2
Augmentations:
RandomResizedCrop
size 224px
interpolation Bilinear Bilinear
RandomHorizontalFlip p= 0.5 p= 0.5
RandomErase p= 0.25 p= 0.25
RandAugment 9/0.5 9/0.5
Color Jitter 0.4 0.4
Frequency masking 12
Table 9. Pretraining hyperparameters
Contrastive loss batch size vs. modalities. While con-
trastive losses do require larger batch size, this requirement
didn’t increase with the number of modalities. As noted
in Appendix B, our experiments (Table 2) sample a mini-
batch of one pair of modalities at a time: batch size of 2K
for (video, audio), and 512 for (image, depth), (image, ther-
mal), and (video, IMU). These batch sizes are smaller than
the>32K batch sizes used in prior work [10, 60].
Combining modalities. In Table 4, we show results with
combining the audio and video modalities. We combine
them by extracting embeddings from both modalities per
sample and computing a linear combinations of those em-
beddings. We used a weight of 0.95 for video and 0.05 for
audio for this combination, which was found to perform the
best.Text query: ”Cooking a meal”
Text query: ”A person doing gardening work outdoors”
Figure 7. IMU retrievals. Given a text query, we show some
IMU retrievals and corresponding video frames.
C.2. Ablation setup
The following setup was used for our evaluations in § 5.
Different from the best setup, all ablation experiments uses
ViT-Base both for the vision and the modality-specific en-
coders. The models are trained for 16 epochs, unless men-
tioned otherwise.
For Table 5b, the differences between the linear and MLP
heads are detailed below: The MLP head did not improve
performance in our experiments.
Linear Linear(in dim, out dim)
MLP Linear(in dim, in dim), GELU, Linear(in dim, out dim)
D. Additional Results
Qualitative results. We show additional results (along with
audio) in the accompanying video.
Practical applications of disparate modalities. In gen-
eral, a shared embedding space enables a variety of differ-
ent cross-modal search and retrieval applications. e.g., since
IMU sensors are ubiquitous (in phones, AR/VR headsets,
health trackers), I MAGE BIND can allow a user to search
an IMU database using text queries (without training with
IMU-text pairs). IMU-based text search has applications
in healthcare/activity search. For instance, in Figure 7 we
show examples of IMU (and accompanying video) retrieval
given textual search query. The retrieved IMU sample,
shown as 3-channel Accelerometer (Acc) and Gyroscope
(Gyro) recording, matches the text query.
E. Additional Ablations
Design choices in losses. Since the modality-specific en-
coders are trained to align with a frozen image encoder, we
tried using a ℓ2regression objective. For ZS SUN top-1
accuracy, we observed that regression led to good perfor-
mance as the sole objective (25.17%) or jointly with con-
trastive (29.04%). However, it did not improve over using
only the contrastive objective (31.74%).
F. Ethical considerations
IMAGE BIND learns a joint embedding for multiple
modalities. Such an embedding is intended to associate se-
mantically related concepts from different modalities. How-
ever, such an embedding may also create unintentional as-
sociations. Thus, joint embedding models, including I M-
AGEBIND must be studied carefully with a lens towards
measuring such associations, and their implications. I M-
AGEBINDleverages the image-text embeddings learned by
a pretrained model on large web-based data which has bi-
ases as documented in different studies [60]. For learning
joint embeddings for other modalities such as audio, ther-
mal, depth, and IMU we leverage datasets mentioned in Ap-
pendix A. These joint embeddings are thus limited to the
concepts present in the datasets. For example, the thermal
datasets we used are limited to outdoor street scenes, while
the depth datasets are limited to indoor scenes.